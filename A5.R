####Assignment 5- Project Themes 11) Microbiome

#For this assignment, I am choosing theme option 11, which will use the package DADA2 to perform amplicon sequencing with marker gene for microbial communities analysis.
#DADA2 adopts a new quality-aware model of Illumina amplicon errors, (As learnt in the lecture,) which presents a relative more robust performance than other software on marker gene analysis for microbiome without constructing OTUs. DADA2 is most sensitive on detecting amplicon sequence variants, and output few false positives. It has better sensitivity on rare species and rare variants (1 or 2 nt difference), so it can retain low-read sequence, and detect true variants that are present in the sample at a low frequency.

#Install and load the required R packages.
#if (!requireNamespace("BiocManager", quietly = TRUE))
#install.packages("BiocManager")
#BiocManager::install(c("dada2", "phyloseq", "ShortRead"))
#install.packages("tidyverse")
#install.packages("vegan")
#install.packages("RColorBrewer")

library(dada2)
library(phyloseq)
library(ShortRead)
library(tidyverse)
library(vegan)
library(Biostrings)
library(ggplot2)
library(ape)
library(RColorBrewer)


#Set the default plotting theme.
theme_set(theme_bw())

#Data acquisition & Data exploration####
#Data set (MiSeq_SOP) used for this assignment is obtained from the DADA2 Pipeline Tutorial by Benjamin J Callahan et al., downloaded from https://www.mothur.org/w/images/d/d6/MiSeqSOPData.zip. The fastq files contain gut bacteria (fecal) samples collected longitudinally from the post-weaning mouse, data were generated by Illumina Miseq amplicon sequencing of the V4 region of the 16S rRNA gene. Each DNA sequence is in 250 bp long. Illumina paired-end sequencing data is demultiplexed.

#Check the fastq files in downloaded folder (MiSeq_SOP).
path <- "~/Desktop/6210 Software Tools/Assignment 5/MiSeq_SOP"
list.files(path)
#These fastq files are paired-end sequencing data in forward and reverse read. All files have the same format: SAMPLENAME_R1_001.fastq and SAMPLENAME_R2_001.fastq. File name with R1 correspond to forward read, file name with R2 correspond to the reverse read.

#Sort files in order, and return the full file name with path.
fnF <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnR <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
fnF[1:5] #The first 5 fastq files for forward reads.

#Extra the 20 sample names form the file names with string manipulation, where sample name is before the first "_".
sample.names <- sapply(strsplit(basename(fnF), "_"), `[`, 1)
sample.names

#Take a look at how fastq file looks like.
fastq <- readLines(fnF[1])
fastq[1:8]
#By viewing the first 8 lines of fastq file, we can find that each entry consist 4 lines of information. The first line is the sequence identifier, second line contains the sequence, the third line is a "+" separator, the last line represent the base-calling quality score of sequence encoded by Phred +33 with ASCII. (illumina, 2021)

#By browsing different package online, I found package "ShortRead" can also be used to read fastq file, it's another way to view fastq file in detail.
rfq1 <- readFastq(fnF[1])
rrq1 <- readFastq(fnR[1])
rfq1
rrq1
#For the first sample, there are 7793 reads in both the forward read and reverse read fastq files.

#To reduce code redundancy, I create a function "fastqdetail" to view the fastq file details.
#First output is the first 6 base sequence in fastq file.
#Second output is the encoded Phred quality score of first 6 reads.
#Third output is the corresponding quality score in numerical value.
fastqdetail <- function(a){
  cat("Sequence:\n")
  sequence <-  sread(a)[1:6]
  print(sequence)
  cat("\nPhred Quality Score:\n")
  quality <-  quality(a)
  print(quality[1:6])
  cat("\nNumerical Quality Score:\n")
  quality_socre <- as(quality,"matrix")[1:2,1:3]
  print(quality_socre)
}


fastqdetail(rfq1) #Detail of the first forward read fastq file.
fastqdetail(rrq1) #Detail of the first reverse read fastq file.

#A Phred quality score of 30 represents a base call error probability of 0.001, accuracy of 99.9%. 30 or higher is typically regarded as good quality. For the downstream analysis, we will need to trim the sequence length based on the quality score of 30 to ensure the algorithm’s sensitivity to rare sequence variants.

#To visualize the quality of forward sequences for first 4 samples, I use function plotQualityProfile from package "dada2".
plotQualityProfile(fnF[1:4])
#Figure 1. Plots of quality score of sequences in the forward read for first 4 samples. The mean quality score at each base position is in green, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.

#For the forward reads, overall quality is good. I will firstly cut off the 10 nucleotides at the beginning, as the quality score are not ideal. Then cut off 10 nts at the end of sequence to avoid less well-controlled errors as Benjamin suggested in the tutorial, length will be set as 240 bp. Therefore, sequence length of 230 bp for forward reads will be obtained after trimming.

#Then check the quality profile of the reverse reads.
plotQualityProfile(fnR[1:4])
#Figure 2. Plots of quality score of sequences in the reverse read for first 4 sample
#Comparatively, forward read sequences have a better performance on quality score drop-off than reserve read sequences, especially towards end of sequence. The quality for all 4 reserve reads files start to drop below 30 around 160 bp position, so I will truncate reverse sequence at 160 bp for the following procedure.


#Sequences trimming and filtering####
#Here I am creating a sub-directory (filtered) in the current path to save filtered fastq files with different  in the compressed format for both forward and reverse reads(compressed file formats are supported).
filtF <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtR <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtF) <- sample.names
names(filtR) <- sample.names

#As the preliminary quality control, I use function filterAndTrim to filter and trim forward and reverse sequences. 
#By this step, forward reads are firstly trimmed at 240 bases, reverse reads are trimmed at 160 bases. For all the reads, sequences with Ns (maxN=0) will be discarded; (maxEE=c(2,2)) expected errors are set as 2, so sequences have more than two errors are filtered out; sequences with quality score lower than 2 will be filtered (truncQ=2); sequences will be trimmed 10 nts from start (trimLeft = 10). the multithread is set as TRUE to reduce the computational time by parallel filtration.
output <- filterAndTrim(fwd = fnF, filt = filtF, rev = fnR, filt.rev = filtR, truncLen=c(240,160), maxN=0, maxEE=c(2,2), truncQ=2, trimLeft = 10, rm.phix=TRUE, compress=TRUE, multithread=TRUE)

head(output)
#Shows the difference of reads before and after filtering and trimming.

#Data dereplication####
#To save computing resource and memory once more, I use derepFastq to dereplicate amplification sequences in fastq file. By this step, identical reads are collapsed together into unique sequences. Consensus quality information and count of each unique sequence are recorded.
derepF <- derepFastq(filtF, verbose=TRUE)
derepR <- derepFastq(filtR, verbose=TRUE)
class(derepF)

#I create another function to check the detailed features of sequences after dereplication. The function "derepdetail" will return the general information of dereplicated sequences, the two most abundant sequences with their count, a statistical summary and a histogram of the quality score profile.
derepdetail <- function(a){
  print(a)
  cat("\nThe Most Abundant Unique Sequences and Their Count:\n")
  uniques <- head(a[["uniques"]],2)
  print(uniques)
  cat("\nThe Summary of Quality Score:\n")
  summary <- summary(as.vector(a[["quals"]]))
  print(summary)
  hist(as.vector(a[["quals"]]), xlab = "Quality Score", ylab="Base/Position", main = "Quality Score Distribution")
}

#length <- length(as.vector(derepF[[1]][["quals"]]))
derepdetail(derepF[[1]])
#Figure 3. The distribution of quality score per base position for forward reads.
#There are 1866 unique sequences out of 7139 forward reads for the first sample. All sequence length is 230 bp as expected.

derepdetail(derepR[[1]])
#Figure 4. The distribution of quality score per base position for reverse reads.
#For reverse reads of first sample, there are 1567 unique sequences, all sequence is 150 bp.

#For both forward reads and reverse reads, mean (35.81, 35.90) and median (37.74, 38) are high as expected, which indicates the previous filtering and trimming step are necessary to ensure the overall quality. Meanwhile, both histogram show that among all, only a few positions are with low quality score (<20).


#Error model inference####
#Following steps are to learn the parameterized error model from filtered and trimmed amplicon sequences (both forward and reverse read).

#I firstly use the undereplicated sequences for error model learning.
errF <- learnErrors(filtF, multithread=TRUE)
errR <- learnErrors(filtR, multithread=TRUE)


#Then I use the dereplicated sequences for error model learning again.
errdF <- learnErrors(derepF, multithread=TRUE)
errdR <- learnErrors(derepR, multithread=TRUE)

all.equal(errF, errdF)
#To check the computing time used for model learning (MacBook Pro, 2017).
system.time(learnErrors(filtF, multithread=TRUE))
#user  system elapsed 
#110.313   2.079  40.335 
system.time(learnErrors(derepF, multithread=TRUE))
#user  system elapsed 
#108.029   1.913  40.352 

#By comparing the model learning process with the original undereplicated sequences and dereplicated sequences, we can find the final error models are the same; whereas, learning the model from dereplicated sequences would reduce the CPU time used for this procedure. Therefore, for the large-scale dataset, it is recommended to use dereplicated sequences to train the error model.

#Then we can visualize the estimated error rate for each possible transition (eg. biological A->C/G/T), in total 16 × 41 transition probabilities.
plotErrors(errdF, nominalQ=TRUE)
#Figure 5. The error information for forward reads. Each point represents the observed error rate for each quality score. The black line represents the error rate estimated by the convergence of algorithmic. The red line shows the expected error rate with the nominal definition of the Q-score.
plotErrors(errdR, nominalQ=TRUE)
#Figure 6. The error information for reverse reads.

#All the plots show a good fit between observed (points) and estimated error rate (black lines), and the error rate decreases as quality score increases, indicating the error models are dependable for following procedures.



#Sample inference####
#Here I am applying function "dada", the core sample inference algorithm "Divisive Amplicon Denoising Algorithm", to the filtered and trimmed sequence data of 20 samples. The "dada" will remove all sequencing errors and return the inferred composition of the samples.
dadaF <- dada(derepF, err = errdF, multithread = TRUE)
#Sample 1 - 7139 reads in 1866 unique sequences.
#Sample 2 - 5314 reads in 1543 unique sequences.
#Sample 3 - 5478 reads in 1415 unique sequences.
#Sample 4 - 2926 reads in 870 unique sequences.
#Sample 5 - 2955 reads in 901 unique sequences.
#Sample 6 - 4323 reads in 1227 unique sequences.
#Sample 7 - 6762 reads in 1690 unique sequences.
#Sample 8 - 4580 reads in 1373 unique sequences.
#Sample 9 - 15695 reads in 3423 unique sequences.
#Sample 10 - 11448 reads in 2629 unique sequences.
#Sample 11 - 12064 reads in 2905 unique sequences.
#Sample 12 - 5054 reads in 1503 unique sequences.
#Sample 13 - 18130 reads in 3493 unique sequences.
#Sample 14 - 6275 reads in 1410 unique sequences.
#Sample 15 - 4068 reads in 1148 unique sequences.
#Sample 16 - 7394 reads in 1760 unique sequences.
#Sample 17 - 4772 reads in 1127 unique sequences.
#Sample 18 - 4890 reads in 1302 unique sequences.
#Sample 19 - 6525 reads in 1613 unique sequences.
#Sample 20 - 4333 reads in 845 unique sequences.
dadaR <- dada(derepR, err = errdR, multithread = TRUE)

dadaF[[1]]
head(dadaF[[1]][["denoised"]],2)
as(dadaF[[1]][["quality"]],"matrix")[1:2,1:3]
#For the first sample, 130 sequence variants were inferred from 1866 input unique sequences (forward reads), abundance of each sequence, and average quality per position are recorded.
dadaR[[1]]
#117 sequence variants were inferred from 1567 input unique sequences (reverse reads).

#Merge paired reads####
#By this step, denoised pair of forward and reverse reads are merged together to produce full sequence, pairs that contain mismatches in the overlap region are discarded. 
mergers <- mergePairs(dadaF, derepF, dadaR, derepR, verbose = TRUE)

head(mergers[[1]]) #Merged results for the first sample.
#Final results are returned as a list of data frames. Each data frame collects the merged unique sequences ($sequence) and their count ($abundance); $forward and $reverse represent the index of the forward/reverse sequence contributed to the merged sequence. Merged sequence = forward-only + exact overlapping + reverse-only portion.
 
#To create an amplicon sequence variant table, sample-by-sequence.
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
#20 samples with 286 true amplicon sequence variants.

#FIXME
#Check the distribution of sequence lengths.
#table(nchar(getSequences(seqtab)))
#Merged sequences' lengths range between 251 to 255, do not differ a lot from the original input data (250bp), which is acceptable. 

#Chimeras removal####
#Chimeras are easily introduced during PCR due to incomplete amplification. As substitution and indel errors are removed by previous denoising, sequences with chimeras will be removed once more by "removeBimeraDenovo".
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
#Identified 52 bimeras out of 286 input sequences.

dim(seqtab.nochim)
chimeras <- (1 - sum(seqtab.nochim)/sum(seqtab))*100
#There are 234 biological amplicon sequence variants (ASV) remained, that chimeras make up about 3.35% of the total merged sequences in 20 samples.


#Track reads through the DADA2 pipeline####
#This step is performed as a sanity check, tracking the number of sequence read throughout the whole pipeline for all samples.
getN <- function(x) sum(getUniques(x))
track <- cbind(output, sapply(dadaF, getN), sapply(dadaR, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
colnames(track) <- c("input", "filtered/trimmed", "denoisedF", "denoisedR", "merged", "nonchimera")
rownames(track) <- sample.names
#View(track)
head(track)
#For each sample, compared to the input data, majority sequences are kept as the true biological sequences after all the quality control steps and the final merge. 

#Taxonomy assignment####
#RC <- rarecurve(seqtab.nochim, xlab = "Number of Individuals Barcoded", ylab = "ASV Abundance")

#To draw a sample-based amplicon sequence variants accumulation curve with vegan package function "specaccum", and check if the sample size and sample completeness are appropriate.
AC <- specaccum(seqtab.nochim)
plot(AC, xlab = "Number of Sample", ylab = "True ASV",main = " Sample-based ASV Accumulation Curve")
#Figure 6. The accumulation curve of true biological amplicon sequence variants vs. 20 samples. 234 biological ASVs are determined from 20 samples. As the curve starts to level off slightly at the end, the sample size would be considered enough for this specific dataset to reveal biodiversity. More data will continue to produce a small number of new ASVs (taxon).

#It is interesting to see that DADA2 provides the naive Bayesian classifier method to assign the taxonomy. I will use function "assignTaxonomy" for the taxonomy assignment of 234 upstream derived ASVs from a training set of reference sequences with known taxonomy.
#Reference database Silva 132 for 16S is obtained from https://zenodo.org/record/1172783#.Ybr6VC_71hA. (Please place Silva 132 files into the same folder as other fastq files.)
taxa <- assignTaxonomy(seqtab.nochim, paste0(path, "/silva_nr_v132_train_set.fa"), multithread=TRUE)
#For species level assignment based on the exact match, add an extra species reference database.
taxa <- addSpecies(taxa, paste0(path, "/silva_species_assignment_v132.fa")) 

dftaxa <- as.data.frame(taxa) 
rownames(dftaxa) <- NULL #To better display the taxon assignment.
head(dftaxa)
sort(unique(dftaxa$Family), decreasing = TRUE)[1:10]
#Most of the 16s rRNA sequences are identified to the family level, very few species are assigned. This is acceptable since sequences from V4 region only contain partial information among all nine hypervariable regions, and reference for mouse gut microbiota is limited in Silva 132 database.


#Evaluate accuracy####
#There is a mock sample in the original input data, with a mixture of sequences from 20 known bacteria strains, which can be used a the reference to assess the accuracy of DADA2 inferred results.

dada.mock <- seqtab.nochim["Mock",]
dada.mock <- sort(dada.mock[dada.mock>0], decreasing = TRUE) #Remove NAs for ASVs in the mock sample
cat("DADA2 inferred", length(dada.mock), "sample sequences present in the Mock community.\n")

#Check if the ASVs identified by DADA2 are the exact matches of the reference sequences in mock community.
mock.ref <- getSequences(file.path(path, "HMP_MOCK.v35.fasta"))
match.ref <- sum(sapply(names(dada.mock), function(x) any(grepl(x, mock.ref))))
#All 20 ASVs determined from DADA2 are exact matches to the reference sequences. The DADA2 analysis on this dataset produces an accuracy of 100%.

#Visulation####
#Following, I will use package "phyloseq" to visualize the taxonomic profile of microbial communities constructed with DADA2 pipeline from post-weaning mouse's fecal samples.

#Gather information that is encoded in sample names.
subject <- sapply(strsplit(sample.names, "D"), `[`, 1)
day <- as.integer(sapply(strsplit(sample.names, "D"), `[`, 2)) #The day of post-weaning.
samdf <- data.frame(Subject=subject, Day=day)
samdf$When <- "Early"
samdf$When[samdf$Day>100] <- "Late"
rownames(samdf) <- sample.names

#Build phyloseq-class object with the ASV table produced by DADA2 pipeline and the taxonomy annotated table.
physeq <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows = FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
physeq <- prune_samples(sample_names(physeq) != "Mock", physeq) #Remove sample of mock community.

#Save ASV sequences to physeq$refseq.
dna <- DNAStringSet(taxa_names(physeq))
names(dna) <- taxa_names(physeq)
physeq <- merge_phyloseq(physeq, dna)
taxa_names(physeq) <- paste0("ASV", seq(ntaxa(physeq)))
physeq
#phyloseq-class experiment-level object
#otu_table()   OTU Table:         [ 234 taxa and 19 samples ]
#sample_data() Sample Data:       [ 19 samples by 2 sample variables ]
#tax_table()   Taxonomy Table:    [ 234 taxa by 7 taxonomic ranks ]
#refseq()      DNAStringSet:      [ 234 reference sequences ]

#Selected top 20 sequences from all 234 sequences, and present their  taxonomic distribution (Phylum/Family).
top20 <- names(sort(taxa_sums(physeq), decreasing = TRUE))[1:20] 
physeq.top20 <- transform_sample_counts(physeq, function(OTU) OTU/sum(OTU))
physeq.top20 <- prune_taxa(top20, physeq.top20)

plot_bar(physeq.top20, fill="Phylum", title = "Microbiome Profile (Phylum)") + facet_wrap(~When, scales="free_x")
#Figure 7. An abundance barplot of Phylum profile of the top 20 sequences (early vs. late sample). Among all Phylum identified, Bacteroidetes has the highest abundance (red).

plot_bar(physeq.top20, fill="Family", title = "Microbiome Profile (Family)") + facet_wrap(~When, scales="free_x")
#Figure 8. Family profile on the first 20 sequences, Muribaculaceae is the most abundant.
#Comparing the early samples and late samples, both taxonomic profile barplots show higher abundance of total bacteria in the late samples.

#Now we can visualize the alpha diversity, Shannon and Simpson measures are used.
plot_richness(physeq, x = "Day", color = "When", measures = c("Shannon", "Simpson"))
#Figure 9. The alpha diversity of samples based on the post-weaning day. The larger the Shannon index, the higher the diversity of bacterial community is. The smaller the Simpson index is, the higher the diversity. No obvious difference on alpha-diversity between early and late samples can be concluded from these 2 figures.

#I then use principal coordinate analysis to ordinate the samples.
physeq.prop <- transform_sample_counts(physeq, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(physeq.prop, method="PCoA")
plot_ordination(physeq.prop , ord.nmds.bray, color="When", title="Principal Coordinate Analysis Plot")
#Figure 10. Principal Coordinate Analysis Plot. Early samples (red) and late samples (blue) formed 2 separate clusters along the x-axis, which indicates a significant difference based on the post-post-weaning days.


#As phyloseq provide all kinds of visualization tools for exploring microbiome profile, I found it is possible to generate phylogenetic trees with function "phy_tree". Therefore, I will try to create a tree for the ASVs inferred from DADA2.
#Firstly, I create a random phylogenetic tree with the "ape" package, and merged the random tree into previous phyloseq-class object.
random_tree = rtree(ntaxa(physeq), rooted=TRUE, tip.label=taxa_names(physeq))
physeq1 = merge_phyloseq(physeq, random_tree)

plot_tree(physeq1, "treeonly", label.tips="taxa_names") + coord_polar(theta = "y")
#Figure 11. A phylogenetic tree of all 234 ASVs identified by DADA2.

#Plot a another phylogenetic tree for the top 20 ASVs.
physeq1.20 = names(sort(taxa_sums(physeq1), decreasing = TRUE)[1:20])
ps20 = prune_taxa(physeq1.20, physeq1)

plot_tree(ps20, color = "Family", shape="Phylum", label.tips= "taxa_names", title = "Phylogenetic Tree for the Top 20 ASVs", text.size = 3) 
#Figure 12. The phylogenetic tree for the Top 20 ASVs.

